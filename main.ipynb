{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diabetes Prediction - Improved Model\n\nTarget: Achieve 70%+ accuracy on leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LassoCV\nfrom imblearn.over_sampling import SMOTE\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"/kaggle/input/playground-series-s5e12/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/playground-series-s5e12/test.csv\")\ndf_sample = pd.read_csv(\"/kaggle/input/playground-series-s5e12/sample_submission.csv\")\n\nprint(f\"Train set shape: {df_train.shape}\")\nprint(f\"Test set shape: {df_test.shape}\")\nprint(f\"\\nFirst few rows:\")\nprint(df_train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\nprint(\"Missing values in train:\", df_train.isnull().sum().sum())\nprint(\"Missing values in test:\", df_test.isnull().sum().sum())\n\n# Target distribution\nprint(\"\\nTarget distribution:\")\nprint(df_train['diagnosed_diabetes'].value_counts())\n\n# Basic statistics\nprint(\"\\nBasic statistics:\")\nprint(df_train.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify feature types\nnumerical_features = df_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\ncategorical_features = df_train.select_dtypes(include=['object']).columns.tolist()\n\n# Remove id and target from features\nif 'diagnosed_diabetes' in numerical_features:\n    numerical_features.remove('diagnosed_diabetes')\nif 'id' in numerical_features:\n    numerical_features.remove('id')\n\nprint(f\"Numerical features: {len(numerical_features)}\")\nprint(f\"Categorical features: {len(categorical_features)}\")\n\n# One-hot encoding for categorical features\ncategorical_for_ohe = [col for col in categorical_features if df_train[col].nunique() > 2]\ndf_train = pd.get_dummies(df_train, columns=categorical_for_ohe, drop_first=True)\ndf_test = pd.get_dummies(df_test, columns=categorical_for_ohe, drop_first=True)\n\n# Align train and test columns\ndf_train, df_test = df_train.align(df_test, join='left', axis=1, fill_value=0)\n\nprint(f\"\\nAfter encoding - Train shape: {df_train.shape}\")\nprint(f\"After encoding - Test shape: {df_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Train/Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_col = \"diagnosed_diabetes\"\nX = df_train.drop(columns=[target_col, \"id\"], errors=\"ignore\")\ny = df_train[target_col]\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nX_test = df_test[X_train.columns]\n\nprint(f\"Train set: {X_train.shape}\")\nprint(f\"Validation set: {X_val.shape}\")\nprint(f\"Test set: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Feature Selection with Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features for Lasso\nscaler_fs = StandardScaler()\nX_train_scaled = scaler_fs.fit_transform(X_train)\nX_val_scaled = scaler_fs.transform(X_val)\nX_test_scaled = scaler_fs.transform(X_test)\n\n# Lasso feature selection\nlasso = LassoCV(cv=5, random_state=42)\nlasso.fit(X_train_scaled, y_train)\n\nlasso_coef = pd.DataFrame({\n    'feature': X_train.columns,\n    'coefficient': np.abs(lasso.coef_)\n}).sort_values('coefficient', ascending=False)\n\nselected_features = lasso_coef[lasso_coef['coefficient'] > 0]['feature'].tolist()\n\nprint(f\"Selected features: {len(selected_features)}\")\nprint(f\"\\nTop 10 features:\\n{lasso_coef.head(10)}\")\n\n# Select features\nX_train_selected = X_train[selected_features]\nX_val_selected = X_val[selected_features]\nX_test_selected = X_test[selected_features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Advanced Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_engineered_features(df_original, df_processed):\n    \"\"\"Add domain-specific engineered features\"\"\"\n    df_eng = df_processed.copy()\n\n    # Cholesterol ratios (important diabetes indicators)\n    df_eng['cholesterol_ratio'] = df_original['cholesterol_total'] / df_original['hdl_cholesterol']\n    df_eng['ldl_hdl_ratio'] = df_original['ldl_cholesterol'] / df_original['hdl_cholesterol']\n    df_eng['tg_hdl_ratio'] = df_original['triglycerides'] / df_original['hdl_cholesterol']\n\n    # Blood pressure features\n    df_eng['bp_diff'] = df_original['systolic_bp'] - df_original['diastolic_bp']\n    df_eng['mean_arterial_pressure'] = (df_original['systolic_bp'] + 2 * df_original['diastolic_bp']) / 3\n\n    # BMI features\n    df_eng['bmi_squared'] = df_original['bmi'] ** 2\n    df_eng['is_obese'] = (df_original['bmi'] >= 30).astype(int)\n    df_eng['is_overweight'] = ((df_original['bmi'] >= 25) & (df_original['bmi'] < 30)).astype(int)\n\n    # Age interactions\n    df_eng['age_bmi_interaction'] = df_original['age'] * df_original['bmi']\n    df_eng['age_squared'] = df_original['age'] ** 2\n\n    # Lifestyle score\n    df_eng['lifestyle_score'] = (\n        df_original['diet_score'] +\n        df_original['physical_activity_minutes_per_week'] / 100 -\n        df_original['alcohol_consumption_per_week'] -\n        df_original['screen_time_hours_per_day']\n    )\n\n    # Health risk score\n    df_eng['health_risk_score'] = (\n        df_original['bmi'] / 10 +\n        df_original['systolic_bp'] / 40 +\n        df_original['cholesterol_total'] / 50 +\n        df_original['age'] / 20\n    )\n\n    # Waist to hip ratio category\n    df_eng['high_waist_hip_ratio'] = (df_original['waist_to_hip_ratio'] > 0.9).astype(int)\n\n    return df_eng\n\n# Apply feature engineering\nX_train_enhanced = add_engineered_features(df_train.loc[X_train_selected.index], X_train_selected)\nX_val_enhanced = add_engineered_features(df_train.loc[X_val_selected.index], X_val_selected)\nX_test_enhanced = add_engineered_features(df_test, X_test_selected)\n\nprint(f\"Enhanced features shape: {X_train_enhanced.shape}\")\nprint(f\"Added {X_train_enhanced.shape[1] - X_train_selected.shape[1]} new features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Model Training\n\n## 7.1 Optimized XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_model = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=8,\n    learning_rate=0.05,\n    min_child_weight=3,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    gamma=0.1,\n    reg_alpha=0.1,\n    reg_lambda=1.0,\n    random_state=42,\n    eval_metric='logloss',\n    n_jobs=-1\n)\n\nxgb_model.fit(\n    X_train_enhanced,\n    y_train,\n    eval_set=[(X_val_enhanced, y_val)],\n    verbose=50\n)\n\ny_pred_xgb = xgb_model.predict(X_val_enhanced)\ny_pred_xgb_proba = xgb_model.predict_proba(X_val_enhanced)[:, 1]\n\nxgb_accuracy = accuracy_score(y_val, y_pred_xgb)\nprint(f\"\\n{'='*50}\")\nprint(f\"XGBoost Validation Accuracy: {xgb_accuracy:.4f}\")\nprint(f\"{'='*50}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lgb_model = lgb.LGBMClassifier(\n    n_estimators=500,\n    max_depth=8,\n    learning_rate=0.05,\n    num_leaves=31,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.1,\n    reg_lambda=1.0,\n    random_state=42,\n    n_jobs=-1,\n    verbose=-1\n)\n\nlgb_model.fit(\n    X_train_enhanced,\n    y_train,\n    eval_set=[(X_val_enhanced, y_val)],\n    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)]\n)\n\ny_pred_lgb = lgb_model.predict(X_val_enhanced)\ny_pred_lgb_proba = lgb_model.predict_proba(X_val_enhanced)[:, 1]\n\nlgb_accuracy = accuracy_score(y_val, y_pred_lgb)\nprint(f\"\\n{'='*50}\")\nprint(f\"LightGBM Validation Accuracy: {lgb_accuracy:.4f}\")\nprint(f\"{'='*50}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3 CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_model = CatBoostClassifier(\n    iterations=500,\n    depth=8,\n    learning_rate=0.05,\n    l2_leaf_reg=3,\n    random_state=42,\n    verbose=50,\n    task_type='CPU'\n)\n\ncat_model.fit(\n    X_train_enhanced,\n    y_train,\n    eval_set=(X_val_enhanced, y_val),\n    verbose=50\n)\n\ny_pred_cat = cat_model.predict(X_val_enhanced)\ny_pred_cat_proba = cat_model.predict_proba(X_val_enhanced)[:, 1]\n\ncat_accuracy = accuracy_score(y_val, y_pred_cat)\nprint(f\"\\n{'='*50}\")\nprint(f\"CatBoost Validation Accuracy: {cat_accuracy:.4f}\")\nprint(f\"{'='*50}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Ensemble - Weighted Average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensemble weights\nweights = {\n    'xgb': 0.35,\n    'lgb': 0.35,\n    'cat': 0.30\n}\n\n# Weighted average of probabilities\nensemble_proba = (\n    weights['xgb'] * y_pred_xgb_proba +\n    weights['lgb'] * y_pred_lgb_proba +\n    weights['cat'] * y_pred_cat_proba\n)\n\n# Convert to predictions\ny_pred_ensemble = (ensemble_proba >= 0.5).astype(int)\n\n# Evaluate ensemble\nensemble_accuracy = accuracy_score(y_val, y_pred_ensemble)\n\n# Compare all models\nprint(f\"\\n{'='*60}\")\nprint(\"MODEL COMPARISON:\")\nprint(f\"{'='*60}\")\nprint(f\"XGBoost:  {xgb_accuracy:.4f}\")\nprint(f\"LightGBM: {lgb_accuracy:.4f}\")\nprint(f\"CatBoost: {cat_accuracy:.4f}\")\nprint(f\"Ensemble: {ensemble_accuracy:.4f}\")\nprint(f\"{'='*60}\")\n\nprint(f\"\\nClassification Report (Ensemble):\")\nprint(classification_report(y_val, y_pred_ensemble))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Generate Test Predictions and Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions on test set\ntest_pred_xgb_proba = xgb_model.predict_proba(X_test_enhanced)[:, 1]\ntest_pred_lgb_proba = lgb_model.predict_proba(X_test_enhanced)[:, 1]\ntest_pred_cat_proba = cat_model.predict_proba(X_test_enhanced)[:, 1]\n\n# Ensemble test predictions\ntest_ensemble_proba = (\n    weights['xgb'] * test_pred_xgb_proba +\n    weights['lgb'] * test_pred_lgb_proba +\n    weights['cat'] * test_pred_cat_proba\n)\n\ntest_ensemble_pred = (test_ensemble_proba >= 0.5).astype(int)\n\n# Create submission file\nsubmission = pd.DataFrame({\n    'id': df_test['id'],\n    'diagnosed_diabetes': test_ensemble_pred\n})\n\n# Save submission\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created!\")\nprint(f\"Shape: {submission.shape}\")\nprint(f\"\\nPrediction distribution:\")\nprint(submission['diagnosed_diabetes'].value_counts())\nprint(f\"\\nFirst few predictions:\")\nprint(submission.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. Additional Improvement Ideas\n\nIf you need to improve accuracy further:\n\n1. **Adjust prediction threshold** - Try values between 0.45-0.55 instead of 0.5\n2. **Optimize ensemble weights** - Test different weight combinations\n3. **Use SMOTE** - Train on balanced data (currently created but not used)\n4. **Cross-validation** - Use StratifiedKFold for more robust validation\n5. **Hyperparameter tuning** - Use Optuna or GridSearchCV for thorough optimization\n6. **More feature engineering** - Create polynomial features, binning, etc.\n7. **Stacking** - Train a meta-learner on top of base model predictions"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}