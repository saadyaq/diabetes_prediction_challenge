{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diabetes Prediction - Optimized Ensemble\n\n**Best so far:** 62.48% \u2192 **Target:** 71%\n\n**Strategy:** Better model diversity + optimized blending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.calibration import CalibratedClassifierCV\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Libraries loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"/kaggle/input/playground-series-s5e12/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/playground-series-s5e12/test.csv\")\n\n# Save IDs\ntest_ids = df_test['id'].copy()\ntarget = df_train['diagnosed_diabetes'].copy()\n\n# Drop ID and target\ndf_train = df_train.drop(['id', 'diagnosed_diabetes'], axis=1)\ndf_test = df_test.drop(['id'], axis=1)\n\n# Encode categorical\ncategorical = df_train.select_dtypes(include=['object']).columns.tolist()\ndf_train = pd.get_dummies(df_train, columns=categorical, drop_first=False)  # Keep all dummies\ndf_test = pd.get_dummies(df_test, columns=categorical, drop_first=False)\n\n# Align\ndf_train, df_test = df_train.align(df_test, join='left', axis=1, fill_value=0)\n\n# Add only proven features\ndf_train['age_bmi'] = df_train['age'] * df_train['bmi']\ndf_train['family_age'] = df_train['family_history_diabetes'] * df_train['age']\n\ndf_test['age_bmi'] = df_test['age'] * df_test['bmi']\ndf_test['family_age'] = df_test['family_history_diabetes'] * df_test['age']\n\nprint(f\"Train: {df_train.shape}, Test: {df_test.shape}\")\nprint(f\"Target balance: {target.value_counts(normalize=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Cross-Validation with Diverse Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_folds = 7  # More folds = more robust\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# OOF predictions for 4 different models\noof_xgb = np.zeros(len(df_train))\noof_lgb = np.zeros(len(df_train))\noof_cat = np.zeros(len(df_train))\noof_rf = np.zeros(len(df_train))\n\n# Test predictions\ntest_xgb = np.zeros(len(df_test))\ntest_lgb = np.zeros(len(df_test))\ntest_cat = np.zeros(len(df_test))\ntest_rf = np.zeros(len(df_test))\n\nprint(f\"Using {n_folds}-fold CV\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Train All Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(skf.split(df_train, target)):\n    print(f\"\\nFold {fold + 1}/{n_folds}\")\n\n    X_tr, X_val = df_train.iloc[train_idx], df_train.iloc[val_idx]\n    y_tr, y_val = target.iloc[train_idx], target.iloc[val_idx]\n\n    # XGBoost - Deeper trees, less regularization\n    xgb_model = xgb.XGBClassifier(\n        n_estimators=400,\n        max_depth=8,\n        learning_rate=0.02,\n        min_child_weight=1,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        gamma=0,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42 + fold,\n        n_jobs=-1\n    )\n    xgb_model.fit(X_tr, y_tr, verbose=False)\n    oof_xgb[val_idx] = xgb_model.predict_proba(X_val)[:, 1]\n    test_xgb += xgb_model.predict_proba(df_test)[:, 1] / n_folds\n\n    # LightGBM - More leaves\n    lgb_model = lgb.LGBMClassifier(\n        n_estimators=400,\n        max_depth=8,\n        learning_rate=0.02,\n        num_leaves=64,\n        subsample=0.85,\n        colsample_bytree=0.85,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=42 + fold,\n        n_jobs=-1,\n        verbose=-1\n    )\n    lgb_model.fit(X_tr, y_tr)\n    oof_lgb[val_idx] = lgb_model.predict_proba(X_val)[:, 1]\n    test_lgb += lgb_model.predict_proba(df_test)[:, 1] / n_folds\n\n    # CatBoost - Different depth\n    cat_model = CatBoostClassifier(\n        iterations=400,\n        depth=7,\n        learning_rate=0.02,\n        l2_leaf_reg=3,\n        random_state=42 + fold,\n        verbose=False\n    )\n    cat_model.fit(X_tr, y_tr)\n    oof_cat[val_idx] = cat_model.predict_proba(X_val)[:, 1]\n    test_cat += cat_model.predict_proba(df_test)[:, 1] / n_folds\n\n    # Random Forest - Different algorithm entirely\n    rf_model = RandomForestClassifier(\n        n_estimators=200,\n        max_depth=12,\n        min_samples_split=10,\n        min_samples_leaf=4,\n        max_features='sqrt',\n        random_state=42 + fold,\n        n_jobs=-1\n    )\n    rf_model.fit(X_tr, y_tr)\n    oof_rf[val_idx] = rf_model.predict_proba(X_val)[:, 1]\n    test_rf += rf_model.predict_proba(df_test)[:, 1] / n_folds\n\nprint(\"\\nAll models trained!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Individual Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_xgb = accuracy_score(target, (oof_xgb > 0.5).astype(int))\nacc_lgb = accuracy_score(target, (oof_lgb > 0.5).astype(int))\nacc_cat = accuracy_score(target, (oof_cat > 0.5).astype(int))\nacc_rf = accuracy_score(target, (oof_rf > 0.5).astype(int))\n\nprint(f\"{'='*50}\")\nprint(\"OOF Accuracy:\")\nprint(f\"{'='*50}\")\nprint(f\"XGBoost:      {acc_xgb:.4f}\")\nprint(f\"LightGBM:     {acc_lgb:.4f}\")\nprint(f\"CatBoost:     {acc_cat:.4f}\")\nprint(f\"RandomForest: {acc_rf:.4f}\")\nprint(f\"{'='*50}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Optimized Blending (Grid Search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import product\n\n# Try different weight combinations\nbest_score = 0\nbest_weights = None\n\n# Grid search over weights (coarse)\nweight_options = [0.1, 0.2, 0.25, 0.3, 0.35, 0.4]\n\nprint(\"Searching for best ensemble weights...\")\nfor w1 in weight_options:\n    for w2 in weight_options:\n        for w3 in weight_options:\n            w4 = 1.0 - w1 - w2 - w3\n            if w4 < 0 or w4 > 0.5:\n                continue\n\n            blend = w1 * oof_xgb + w2 * oof_lgb + w3 * oof_cat + w4 * oof_rf\n            acc = accuracy_score(target, (blend > 0.5).astype(int))\n\n            if acc > best_score:\n                best_score = acc\n                best_weights = (w1, w2, w3, w4)\n\nprint(f\"\\nBest weights: XGB={best_weights[0]:.2f}, LGB={best_weights[1]:.2f}, CAT={best_weights[2]:.2f}, RF={best_weights[3]:.2f}\")\nprint(f\"Best OOF accuracy: {best_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Generate Final Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply best weights to test predictions\nfinal_test_proba = (\n    best_weights[0] * test_xgb +\n    best_weights[1] * test_lgb +\n    best_weights[2] * test_cat +\n    best_weights[3] * test_rf\n)\n\n# Try multiple thresholds\nfor thresh in [0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53]:\n    preds = (final_test_proba > thresh).astype(int)\n    submission = pd.DataFrame({\n        'id': test_ids,\n        'diagnosed_diabetes': preds\n    })\n    filename = f'submission_{int(thresh*100)}.csv'\n    submission.to_csv(filename, index=False)\n    print(f\"{filename}: {preds.mean():.3f} positive rate\")\n\nprint(\"\\nAll submissions created!\")\nprint(\"Try submission_50.csv first, then others if needed\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}