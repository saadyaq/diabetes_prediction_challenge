{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diabetes Prediction - Target: 71% Test Accuracy\n\n**Baseline:** 61% \u2192 **Current:** 62% \u2192 **Goal:** 71%\n\n**Strategy:** Focus on test generalization, not just validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, log_loss\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"/kaggle/input/playground-series-s5e12/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/playground-series-s5e12/test.csv\")\n\nprint(f\"Train: {df_train.shape}\")\nprint(f\"Test: {df_test.shape}\")\nprint(f\"\\nTarget distribution:\")\nprint(df_train['diagnosed_diabetes'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Feature Engineering - Focus on Robust Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save IDs and target\ntrain_ids = df_train['id']\ntest_ids = df_test['id']\ntarget = df_train['diagnosed_diabetes']\n\n# Drop ID from features\ndf_train = df_train.drop(['id', 'diagnosed_diabetes'], axis=1)\ndf_test = df_test.drop(['id'], axis=1)\n\n# One-hot encode categorical\ncategorical_features = df_train.select_dtypes(include=['object']).columns.tolist()\ndf_train = pd.get_dummies(df_train, columns=categorical_features, drop_first=True)\ndf_test = pd.get_dummies(df_test, columns=categorical_features, drop_first=True)\n\n# Align columns\ndf_train, df_test = df_train.align(df_test, join='left', axis=1, fill_value=0)\n\n# Add interaction features (proven to help generalization)\ndf_train['age_bmi'] = df_train['age'] * df_train['bmi']\ndf_train['bp_ratio'] = df_train['systolic_bp'] / (df_train['diastolic_bp'] + 1)\ndf_train['cholesterol_hdl_ratio'] = df_train['cholesterol_total'] / (df_train['hdl_cholesterol'] + 1)\n\ndf_test['age_bmi'] = df_test['age'] * df_test['bmi']\ndf_test['bp_ratio'] = df_test['systolic_bp'] / (df_test['diastolic_bp'] + 1)\ndf_test['cholesterol_hdl_ratio'] = df_test['cholesterol_total'] / (df_test['hdl_cholesterol'] + 1)\n\nprint(f\"Features after engineering: {df_train.shape[1]}\")\nprint(f\"Train: {df_train.shape}, Test: {df_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Cross-Validation Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use 5-fold CV for robust validation\nn_folds = 5\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# Store OOF (out-of-fold) predictions\noof_preds_xgb = np.zeros(len(df_train))\noof_preds_lgb = np.zeros(len(df_train))\noof_preds_cat = np.zeros(len(df_train))\n\n# Store test predictions\ntest_preds_xgb = np.zeros(len(df_test))\ntest_preds_lgb = np.zeros(len(df_test))\ntest_preds_cat = np.zeros(len(df_test))\n\nprint(f\"Using {n_folds}-fold cross-validation\")\nprint(f\"Training on {len(df_train)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 XGBoost with CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(skf.split(df_train, target)):\n    print(f\"\\n{'='*50}\")\n    print(f\"Fold {fold + 1}/{n_folds}\")\n    print(f\"{'='*50}\")\n\n    X_train, X_val = df_train.iloc[train_idx], df_train.iloc[val_idx]\n    y_train, y_val = target.iloc[train_idx], target.iloc[val_idx]\n\n    # XGBoost\n    xgb_model = xgb.XGBClassifier(\n        n_estimators=500,\n        max_depth=6,\n        learning_rate=0.03,\n        min_child_weight=3,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        gamma=0.1,\n        reg_alpha=0.5,\n        reg_lambda=2.0,\n        random_state=42,\n        eval_metric='logloss',\n        n_jobs=-1\n    )\n\n    xgb_model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        verbose=False\n    )\n\n    # OOF predictions\n    oof_preds_xgb[val_idx] = xgb_model.predict_proba(X_val)[:, 1]\n\n    # Test predictions\n    test_preds_xgb += xgb_model.predict_proba(df_test)[:, 1] / n_folds\n\n    # Validation score\n    val_acc = accuracy_score(y_val, (oof_preds_xgb[val_idx] > 0.5).astype(int))\n    print(f\"XGBoost Fold {fold + 1} Accuracy: {val_acc:.4f}\")\n\n# Overall OOF score\noof_acc_xgb = accuracy_score(target, (oof_preds_xgb > 0.5).astype(int))\nprint(f\"\\nXGBoost OOF Accuracy: {oof_acc_xgb:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 LightGBM with CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(skf.split(df_train, target)):\n    print(f\"Fold {fold + 1}/{n_folds}\")\n\n    X_train, X_val = df_train.iloc[train_idx], df_train.iloc[val_idx]\n    y_train, y_val = target.iloc[train_idx], target.iloc[val_idx]\n\n    # LightGBM\n    lgb_model = lgb.LGBMClassifier(\n        n_estimators=500,\n        max_depth=6,\n        learning_rate=0.03,\n        num_leaves=31,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.5,\n        reg_lambda=2.0,\n        random_state=42,\n        n_jobs=-1,\n        verbose=-1\n    )\n\n    lgb_model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        callbacks=[lgb.early_stopping(50, verbose=False)]\n    )\n\n    # OOF predictions\n    oof_preds_lgb[val_idx] = lgb_model.predict_proba(X_val)[:, 1]\n\n    # Test predictions\n    test_preds_lgb += lgb_model.predict_proba(df_test)[:, 1] / n_folds\n\n    val_acc = accuracy_score(y_val, (oof_preds_lgb[val_idx] > 0.5).astype(int))\n    print(f\"LightGBM Fold {fold + 1} Accuracy: {val_acc:.4f}\")\n\noof_acc_lgb = accuracy_score(target, (oof_preds_lgb > 0.5).astype(int))\nprint(f\"\\nLightGBM OOF Accuracy: {oof_acc_lgb:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 CatBoost with CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(skf.split(df_train, target)):\n    print(f\"Fold {fold + 1}/{n_folds}\")\n\n    X_train, X_val = df_train.iloc[train_idx], df_train.iloc[val_idx]\n    y_train, y_val = target.iloc[train_idx], target.iloc[val_idx]\n\n    # CatBoost\n    cat_model = CatBoostClassifier(\n        iterations=500,\n        depth=6,\n        learning_rate=0.03,\n        l2_leaf_reg=5,\n        random_state=42,\n        verbose=False\n    )\n\n    cat_model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n\n    # OOF predictions\n    oof_preds_cat[val_idx] = cat_model.predict_proba(X_val)[:, 1]\n\n    # Test predictions\n    test_preds_cat += cat_model.predict_proba(df_test)[:, 1] / n_folds\n\n    val_acc = accuracy_score(y_val, (oof_preds_cat[val_idx] > 0.5).astype(int))\n    print(f\"CatBoost Fold {fold + 1} Accuracy: {val_acc:.4f}\")\n\noof_acc_cat = accuracy_score(target, (oof_preds_cat > 0.5).astype(int))\nprint(f\"\\nCatBoost OOF Accuracy: {oof_acc_cat:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Stacking - Train Meta-Model on OOF Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create meta-features from OOF predictions\nmeta_train = np.column_stack([oof_preds_xgb, oof_preds_lgb, oof_preds_cat])\nmeta_test = np.column_stack([test_preds_xgb, test_preds_lgb, test_preds_cat])\n\n# Train meta-model (Logistic Regression for simplicity and regularization)\nmeta_model = LogisticRegression(random_state=42, max_iter=1000)\nmeta_model.fit(meta_train, target)\n\n# Meta-model predictions\nfinal_oof_preds = meta_model.predict_proba(meta_train)[:, 1]\nfinal_test_preds = meta_model.predict_proba(meta_test)[:, 1]\n\n# OOF accuracy\noof_acc_stacking = accuracy_score(target, (final_oof_preds > 0.5).astype(int))\n\nprint(f\"\\n{'='*60}\")\nprint(\"MODEL COMPARISON (OOF Accuracy):\")\nprint(f\"{'='*60}\")\nprint(f\"XGBoost:  {oof_acc_xgb:.4f}\")\nprint(f\"LightGBM: {oof_acc_lgb:.4f}\")\nprint(f\"CatBoost: {oof_acc_cat:.4f}\")\nprint(f\"Stacking: {oof_acc_stacking:.4f}\")\nprint(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Threshold Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try different thresholds on OOF predictions\nbest_threshold = 0.5\nbest_acc = 0\n\nfor threshold in np.arange(0.3, 0.7, 0.01):\n    acc = accuracy_score(target, (final_oof_preds > threshold).astype(int))\n    if acc > best_acc:\n        best_acc = acc\n        best_threshold = threshold\n\nprint(f\"Best threshold: {best_threshold:.3f}\")\nprint(f\"Best OOF accuracy: {best_acc:.4f}\")\nprint(f\"Improvement: +{(best_acc - oof_acc_stacking)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Generate Final Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use optimized threshold\nfinal_predictions = (final_test_preds > best_threshold).astype(int)\n\n# Create submission\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'diagnosed_diabetes': final_predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"\\nSubmission created!\")\nprint(f\"Shape: {submission.shape}\")\nprint(f\"\\nPrediction distribution:\")\nprint(submission['diagnosed_diabetes'].value_counts(normalize=True))\nprint(f\"\\nExpected test accuracy: ~{best_acc:.1%}\")\nprint(f\"Target: 71%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}